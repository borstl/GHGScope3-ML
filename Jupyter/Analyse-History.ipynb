{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from data.cleaning import read_all_historic_csv\n",
    "from core import Config\n",
    "import pandas as pd\n",
    "\n",
    "config = Config()\n",
    "historic_dictionary: dict[str, pd.DataFrame] = read_all_historic_csv(config.filtered_dir_historic)\n",
    "all_historic_frame: pd.DataFrame = pd.concat(historic_dictionary.values(), names=[0, 1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explorative Data Analysis",
   "id": "bf7e3c671bdc43ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_historic_frame.info()",
   "id": "a31b34be6a2505d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filter out every company without reporting Scope 3.1 properly",
   "id": "3db679c771d144fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scope31: pd.Series = all_historic_frame[\"TR.UpstreamScope3PurchasedGoodsAndServices\"]\n",
    "# list every index that has only NaN values\n",
    "nan_index = scope31.groupby(level=0).filter(lambda x: x.isna().sum() == len(x))\n",
    "all_historic_frame.drop(index=nan_index.index, inplace=True)"
   ],
   "id": "67abd1c2e21414f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detect all features that have most companies in common",
   "id": "21f243edf4215a40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "historic_counter: Counter = Counter()\n",
    "for dataframe in historic_dictionary.values():\n",
    "    historic_counter.update(dataframe.columns.to_list())\n",
    "most_common_historic_columns: list[tuple[str, int]] = historic_counter.most_common()\n",
    "historic_dictionary_len = len(historic_dictionary)\n",
    "del historic_dictionary"
   ],
   "id": "dbbebf967e6edc4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# figure out the desired Scope 3.1 purchased goods and services value and how it was reported\n",
    "scope3_1_count_per_year: pd.Series = (\n",
    "    all_historic_frame[\"TR.UpstreamScope3PurchasedGoodsAndServices\"].notna().groupby(level=1).sum()\n",
    ")\n",
    "fig = px.bar(\n",
    "    scope3_1_count_per_year,\n",
    "    x=scope3_1_count_per_year.index,\n",
    "    y=\"TR.UpstreamScope3PurchasedGoodsAndServices\",\n",
    "    color=\"TR.UpstreamScope3PurchasedGoodsAndServices\",\n",
    "    color_continuous_scale=\"mint\",\n",
    "    text_auto=True,\n",
    "    labels={\n",
    "        \"TR.UpstreamScope3PurchasedGoodsAndServices\": \"Anzahl Reports von Scope 3.1\",\n",
    "        \"Date\": \"Jahr\",\n",
    "    }\n",
    ")\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.write_image(config.data_dir / \"figures\" / \"scope3_1_count_per_year.png\")\n",
    "fig.show()"
   ],
   "id": "a6762126bb43123b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When inspecting the Data for how many companies reported Scope 3.1, then we can see that only from 2016 the first values are reported. This gradually rises up to 2023 with 2705 companies and falls down to 1431 in 2024, probably because they didn't have the time to disclose the value yet. This leads to many data to be properly imputed, because this is our targed value.",
   "id": "565f0d64ae3b2087"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_date = pd.Timestamp('2016-01-01')\n",
    "end_date = pd.Timestamp('2024-12-31')\n",
    "mask = (all_historic_frame.index.get_level_values('Date') >= start_date) & (\n",
    "            all_historic_frame.index.get_level_values('Date') <= end_date)\n",
    "all_historic_frame = all_historic_frame[mask]"
   ],
   "id": "f0e3e87d6bb97197",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bar_data: pd.DataFrame = pd.DataFrame(most_common_historic_columns, columns=[\"Features\", \"sameFeatureCount\"])\n",
    "bar_data[\"sameFeatureCount\"] = bar_data[\"sameFeatureCount\"].apply(lambda x: x / historic_dictionary_len * 100)\n",
    "labels = [\"{0} - {1}\".format(i, i + 10) for i in range(0, 100, 10)]\n",
    "bar_data[\"companyCountIn%\"] = pd.cut(bar_data[\"sameFeatureCount\"], range(0, 101, 10), labels=labels, right=True)\n",
    "grouped: pd.DataFrame = bar_data.groupby(\"companyCountIn%\").count().reset_index(names=\"companyCountIn%\")\n",
    "grouped[\"companyCountIn%\"] = grouped[\"companyCountIn%\"].astype(str)\n",
    "mask = bar_data.loc[bar_data[\"sameFeatureCount\"] < 90, \"Features\"]"
   ],
   "id": "8a957522283125fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.bar(\n",
    "    grouped,\n",
    "    x=\"companyCountIn%\",\n",
    "    y=\"sameFeatureCount\",\n",
    "    color=\"sameFeatureCount\",\n",
    "    color_continuous_scale=\"mint\",\n",
    "    text_auto=True,\n",
    "    labels={\n",
    "        \"companyCountIn%\": \"Anzahl Unternehmen in %\",\n",
    "        \"sameFeatureCount\": \"Anzahl überschneidender zeitreihen Features\",\n",
    "    }\n",
    ")\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.write_image(config.data_dir / \"figures\" / \"historic-same-features-count.png\")\n",
    "fig.show()"
   ],
   "id": "bd5eb12b379e8552",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bar_data: pd.DataFrame = pd.DataFrame(most_common_historic_columns, columns=[\"Features\", \"sameFeatureCount\"])\n",
    "bar_data[\"sameFeatureCount\"] = bar_data[\"sameFeatureCount\"].apply(lambda x: int(x / historic_dictionary_len * 100))\n",
    "labels = [\"{0}\".format(i) for i in range(91, 101, 1)]\n",
    "bar_data[\"companyCountIn%\"] = pd.cut(bar_data[\"sameFeatureCount\"], range(90, 101, 1), labels=labels)\n",
    "grouped: pd.DataFrame = bar_data.groupby(\"companyCountIn%\").count().reset_index(names=\"companyCountIn%\")\n",
    "grouped[\"companyCountIn%\"] = grouped[\"companyCountIn%\"].astype(str)\n",
    "mask_under_90 = bar_data.loc[bar_data[\"sameFeatureCount\"] < 90, \"Features\"]\n",
    "mask_90_97 = bar_data.loc[(bar_data[\"sameFeatureCount\"] >= 90) & (bar_data[\"sameFeatureCount\"] <= 98), \"Features\"]\n",
    "mask_99_100 = bar_data.loc[bar_data[\"sameFeatureCount\"] > 98, \"Features\"]"
   ],
   "id": "5a4d97147dbc3aea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.bar(\n",
    "    grouped,\n",
    "    x=\"companyCountIn%\",\n",
    "    y=\"sameFeatureCount\",\n",
    "    color=\"sameFeatureCount\",\n",
    "    color_continuous_scale=\"mint\",\n",
    "    text_auto=True,\n",
    "    labels={\n",
    "        \"sameFeatureCount\": \"Anzahl überschneidender statischer Features\",\n",
    "        \"companyCountIn%\": \"Anzahl Unternehmen in %\"\n",
    "    },\n",
    ")\n",
    "fig.update_traces(cliponaxis=False)\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.show()"
   ],
   "id": "ef75f2fbb9717c0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The companies are not reporting all features equally. Only 623 features are reported by 99% or more companies. 709 Features are reported by over 90% of all companies.",
   "id": "2e84210875830f5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Domain Knowledge Filtering",
   "id": "7fcebb330c767d0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# features that should remain in the subset\n",
    "remainder: list[str] = [\n",
    "    'TR.NumberofEmployees',\n",
    "    'TR.ThermalCoalfiredPowerGeneration'\n",
    "]"
   ],
   "id": "59815a58056bd9a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_historic_frame.drop(columns=mask_under_90, inplace=True)\n",
    "all_historic_frame2 = all_historic_frame[mask_90_97]\n",
    "all_historic_frame3 = all_historic_frame[mask_99_100]"
   ],
   "id": "7b01149f9d4d129a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_historic_frame = all_historic_frame3.convert_dtypes()\n",
    "all_historic_frame.to_csv(config.eda_filtered_dir / \"eda_filtered_historic.csv\")\n",
    "#safe dtypes as files to make it possible to know the dtypes of the file while reading\n",
    "dtypes: pd.DataFrame = all_historic_frame.dtypes.to_frame('dtypes').reset_index()\n",
    "dtypes.to_csv(config.eda_filtered_dir / \"eda_filtered_historic_dtypes.csv\")"
   ],
   "id": "5b95f301b1476b91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Working with filtered Dataset",
   "id": "6996d59472d57517"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:47:11.348497Z",
     "start_time": "2026-02-16T10:47:07.713090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core import Config\n",
    "import pandas as pd\n",
    "\n",
    "config = Config()\n",
    "\n",
    "dtypes: pd.DataFrame = pd.read_csv(\n",
    "    config.eda_filtered_dir / \"eda_filtered_historic_dtypes.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "dtypes_dict: dict[str, str] = {}\n",
    "for row in dtypes.itertuples(index=False):\n",
    "    dtypes_dict[row[0]] = row[1]\n",
    "\n",
    "all_historic_frame = pd.read_csv(\n",
    "    config.eda_filtered_dir / \"eda_filtered_historic.csv\",\n",
    "    dtype=dtypes_dict,\n",
    "    index_col=[0, 1]\n",
    ")\n",
    "\n",
    "reference_filtered_frame = all_historic_frame.copy()"
   ],
   "id": "970ee120754c8d07",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:47:35.461375Z",
     "start_time": "2026-02-16T10:47:11.356571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instrument_group = all_historic_frame.groupby(level=0)\n",
    "historic_dictionary: dict[str, pd.DataFrame] = {}\n",
    "for key in instrument_group.groups:\n",
    "    historic_dictionary[str(key)] = all_historic_frame.loc[str(key)]"
   ],
   "id": "2748b280926e1d10",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Impute values",
   "id": "7ea34dd6af2da710"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:47:35.540408Z",
     "start_time": "2026-02-16T10:47:35.502911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Detect columns with only boolean-like strings and convert dtype to boolean\n",
    "bool_strings = [{'true', 'false'}, {'True', 'False'}, {'yes', 'no'}, {'Yes', 'No'}, {'1', '0'}]\n",
    "def is_bool_string_col(series):\n",
    "    values = set(series.dropna().unique())\n",
    "    return any(values <= s for s in bool_strings)\n",
    "\n",
    "for col in all_historic_frame.select_dtypes(include=['object', 'string']):\n",
    "    if is_bool_string_col(all_historic_frame[col]):\n",
    "        all_historic_frame[col] = all_historic_frame[col].replace({'true': True, 'True': True, 'yes': True, 'Yes': True, '1': True,\n",
    "                                  'false': False, 'False': False, 'no': False, 'No': False, '0': False}).astype('boolean')\n",
    "        all_historic_frame[col] = all_historic_frame[col].astype('boolean')  # use 'bool' for non-nullable booleans"
   ],
   "id": "22580dcc30f765da",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:47:35.552502Z",
     "start_time": "2026-02-16T10:47:35.542310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = all_historic_frame.columns.to_series().groupby(all_historic_frame.dtypes.apply(lambda x: x.name))\n",
    "boolean_cols = all_historic_frame.select_dtypes('bool')"
   ],
   "id": "a57966522e5f889d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:47:35.682035Z",
     "start_time": "2026-02-16T10:47:35.554044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#remove columns with only one unique value\n",
    "is_single_value = [\n",
    "    col for col in all_historic_frame\n",
    "    if all_historic_frame[col].nunique(dropna=True) == 1\n",
    "]\n",
    "all_historic_frame.drop(is_single_value, axis=1, inplace=True)"
   ],
   "id": "438d531eaa7cb711",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import core.config as config\n",
    "config = config.Config()\n",
    "column_names: list[str] = all_historic_frame.columns.values.tolist()\n",
    "column_names.sort()\n",
    "with open(config.eda_features_file_historic, \"w\") as file:\n",
    "    file.write(\"\\n\".join(str(i) for i in column_names))"
   ],
   "id": "ebd2bed9ee2d75fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count NaN in the whole time series frame and filter\n",
    "nan_count: pd.Series = all_historic_frame.isna().sum()\n",
    "# filter how many NaN per year\n",
    "nan_per_year: pd.DataFrame = all_historic_frame.isna().groupby(level=1).sum()\n",
    "nan_per_year_and_column: pd.Series = nan_per_year.sum(axis=1)\n",
    "# filter features with threshold NaNs TODO check the use of this threshold\n",
    "filtered: pd.DataFrame = all_historic_frame.drop(nan_count.index[nan_count > 15000], axis=1)"
   ],
   "id": "d462b40813c82363",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:47:35.867889Z",
     "start_time": "2026-02-16T10:47:35.688997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a series of every company with the number of distinct values in the TR.UpstreamScope3PurchasedGoodsAndServices column.\n",
    "scope31_count: list[int] = []\n",
    "for key, df in historic_dictionary.items():\n",
    "    scope31_count.append(df[\"TR.UpstreamScope3PurchasedGoodsAndServices\"].nunique())\n",
    "# TODO get to know if there are continuously the same values repeatedly reported"
   ],
   "id": "3d9398a47f13c736",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Remove every row, where TR.UpstreamScope3PurchasedGoodsAndServices is NaN",
   "id": "68b21bb14a24bc73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:47:35.989484Z",
     "start_time": "2026-02-16T10:47:35.874847Z"
    }
   },
   "cell_type": "code",
   "source": "all_historic_frame.drop(all_historic_frame[all_historic_frame[\"TR.UpstreamScope3PurchasedGoodsAndServices\"].isna()].index, inplace=True)",
   "id": "5255f878529b1d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline Imputation: Remove columns with many missing values, use median for numerical and mode for categorical data",
   "id": "4ff1dd6fa73245f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T10:53:11.951907Z",
     "start_time": "2026-02-16T10:47:35.996543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check how many columns have NaN values\n",
    "nan_count = all_historic_frame.isna().sum()\n",
    "# every numerical columns median per column and group\n",
    "instrument_group = all_historic_frame.groupby(level=0)\n",
    "historic_dictionary: dict[str, pd.DataFrame] = {}\n",
    "for key in instrument_group.groups:\n",
    "    historic_dictionary[str(key)] = all_historic_frame.loc[str(key)]\n",
    "# Median for columns with numeric types\n",
    "for instrument in historic_dictionary.keys():\n",
    "    # float\n",
    "    for col in all_historic_frame.select_dtypes('float64').columns:\n",
    "        median = all_historic_frame[col].median()\n",
    "        all_historic_frame[col] = all_historic_frame[col].fillna(median)\n",
    "    # int\n",
    "    for col in all_historic_frame.select_dtypes('int64').columns:\n",
    "        median = all_historic_frame[col].median().round().astype(\"int64\")\n",
    "        all_historic_frame[col] = all_historic_frame[col].fillna(median)\n",
    "\n",
    "# Mode for columns with categorical types\n",
    "for instrument in historic_dictionary.keys():\n",
    "    for col in all_historic_frame.select_dtypes(['string', 'boolean']).columns:\n",
    "        mode = all_historic_frame[col].mode()[0]\n",
    "        all_historic_frame[col] = all_historic_frame[col].fillna(mode)"
   ],
   "id": "1bd5e809e4567190",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T11:34:05.971884Z",
     "start_time": "2026-02-16T11:34:03.728275Z"
    }
   },
   "cell_type": "code",
   "source": "all_historic_frame.to_csv(config.median_historic)",
   "id": "838a1c136e018d33",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove Data with too many NaN",
   "id": "a2b4b83e16ec0791"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historic_frame: pd.DataFrame = all_historic_frame.reset_index()",
   "id": "20eea8857afacc0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Keep rows with at least 90% of the columns filled\n",
    "historic_frame2 = historic_frame.dropna(thresh=int(len(all_historic_frame.columns) * 0.90))"
   ],
   "id": "7035976913cbef0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diagnosing Mechanism",
   "id": "b938ed9ce4e5f41f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "col = historic_frame.isna().sum().sort_values(ascending=False)\n",
    "# identify rows that have the highest number of NaN values\n",
    "NaN_row_count = historic_frame[col.index].isna().sum(axis=1).sort_values(ascending=False)\n",
    "# proportion of missing values\n",
    "NaN_count = historic_frame.isna().sum()\n",
    "NaN_prop: pd.Series = historic_frame.isna().sum() / len(historic_frame)"
   ],
   "id": "f0175e68632420e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(historic_frame2[col.index].isna().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})\n",
    "plt.savefig(\"visualizing_missing_data_with_heatmap_Seaborn_Python.png\", dpi=300)"
   ],
   "id": "124b3e350f538296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Don't impute Scope 3.1 the target variable",
   "id": "db04522422399fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "scope31: pd.DataFrame = all_historic_frame[\"TR.UpstreamScope3PurchasedGoodsAndServices\"].to_frame()\n",
    "scope31.dropna(inplace=True)\n",
    "def normalize_values(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (frame - frame.min()) / (frame.max() - frame.min())\n",
    "g = scope31.groupby(level=0)[\"TR.UpstreamScope3PurchasedGoodsAndServices\"]\n",
    "scope31['Scope31Norm'] = g.transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "subset = scope31.head(300).reset_index()\n",
    "\n",
    "fig = px.line(\n",
    "    subset,\n",
    "    x=1,\n",
    "    y='Scope31Norm',\n",
    "    color=0,\n",
    "    title='Data Trends Over Years',\n",
    "    template='plotly_dark',\n",
    "    labels={'Scope31Norm': 'Normalized Value', '0': 'Company', '1': 'Year'}\n",
    ")\n",
    "fig.show()"
   ],
   "id": "197845bc6bfd8430",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agg = scope31.groupby(1)[\"Scope31Norm\"].median().reset_index()\n",
    "px.line(agg, x=1, y=\"Scope31Norm\", title=\"Median Value per Year\").show()"
   ],
   "id": "2dac7f991bdd34e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
