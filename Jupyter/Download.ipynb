{
 "cells": [
  {
   "metadata": {
    "tags": [
     "Initialization"
    ]
   },
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "Configuration, Logger, Counter and Downloader"
   ],
   "id": "b2f2a0d6e7bd322b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:22:03.368700Z",
     "start_time": "2025-10-24T07:22:02.804484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from core.config import Config, split_in_chunks\n",
    "from data.download import LSEGDataDownloader\n",
    "\n",
    "os.environ[\"RD_LIB_CONFIG_PATH\"] = \"/Configuration\"\n",
    "\n",
    "config = Config()\n",
    "logging.basicConfig(\n",
    "        filename=config.log_file,\n",
    "        encoding=\"utf-8\",\n",
    "        level=config.log_level,\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "        datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "logger = logging.getLogger()\n",
    "\n",
    "RAW_DATA_PATH: pathlib.Path = config.full_dir_raw_data\n",
    "STATIC_DATA_PATH: pathlib.Path = config.full_dir_static\n",
    "HISTORIC_DATA_PATH: pathlib.Path = config.full_dir_historic\n",
    "# In config under post_init change the path to which features are getting loaded"
   ],
   "id": "e74ca2369a377986",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Downloading time series Data",
   "id": "d595c661a33d1b36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:22:29.537217Z",
     "start_time": "2025-10-24T07:22:05.058055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.exceptions import DataValidationError, DataDownloadError\n",
    "\n",
    "with (LSEGDataDownloader(config) as downloader):\n",
    "    for i, company_chunk in enumerate(config.companies_historic_chunks):\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                print(f\"Downloading historic data: for {company_chunk[0]} to {company_chunk[-1]}\")\n",
    "                standardized_histordict: dict[str, pd.DataFrame] = {}\n",
    "                if len(config.historic_features) >= 1000:\n",
    "                    standardized_histordict = downloader.download_historic_in_chunks(\n",
    "                        companies=company_chunk,\n",
    "                        raw_data_dir=RAW_DATA_PATH,\n",
    "                    )\n",
    "                else:\n",
    "                    standardized_histordict = downloader.download_historic_from(\n",
    "                        companies=company_chunk,\n",
    "                        chunk=config.historic_features,\n",
    "                        raw_data_dir=RAW_DATA_PATH,\n",
    "                        iteration= i\n",
    "                    )\n",
    "                for key, new_df in standardized_histordict.items():\n",
    "                    new_df.to_csv(HISTORIC_DATA_PATH / f\"company-{key}.csv\")\n",
    "                break\n",
    "            except DataValidationError as e:\n",
    "                company_chunk.remove(e.__getcompanies__())\n",
    "                with open(config.removed_companies_file, \"a\") as file:\n",
    "                    file.write(f\"\\n{e.__getcompanies__()}\")\n",
    "                print(f\"Removed {e.__getcompanies__()} Try again.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading historic data: {company_chunk}\")\n",
    "        else:\n",
    "            raise DataDownloadError(\n",
    "                \"Too many attempts to download historic data\",\n",
    "                company_chunk,\n",
    "                len(config.historic_features)\n",
    "            )"
   ],
   "id": "8ea3a8e6266ce6f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading historic data: for 000063.SZ to 000880.KS\n",
      "Downloading Chunk 1:4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m standardized_histordict: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, pd.DataFrame] = {}\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(config.historic_features) >= \u001B[32m1000\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     standardized_histordict = \u001B[43mdownloader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdownload_historic_in_chunks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompanies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompany_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[43mraw_data_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mRAW_DATA_PATH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     15\u001B[39m     standardized_histordict = downloader.download_historic_from(\n\u001B[32m     16\u001B[39m         companies=company_chunk,\n\u001B[32m     17\u001B[39m         chunk=config.historic_features,\n\u001B[32m     18\u001B[39m         raw_data_dir=RAW_DATA_PATH,\n\u001B[32m     19\u001B[39m         iteration= i\n\u001B[32m     20\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML/src/data/download.py:153\u001B[39m, in \u001B[36mLSEGDataDownloader.download_historic_in_chunks\u001B[39m\u001B[34m(self, companies, raw_data_dir)\u001B[39m\n\u001B[32m    150\u001B[39m msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDownloading Chunk \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.config.historic_chunks)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;28mprint\u001B[39m(msg)\n\u001B[32m    152\u001B[39m standardized_data: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, pd.DataFrame] = (\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdownload_historic_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompanies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhistoric_chunks\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_data_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[32m    154\u001B[39m collection: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, pd.DataFrame] = standardized_data\n\u001B[32m    155\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.config.historic_chunks[\u001B[32m1\u001B[39m:]):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML/src/data/download.py:173\u001B[39m, in \u001B[36mLSEGDataDownloader.download_historic_from\u001B[39m\u001B[34m(self, companies, chunk, raw_data_dir, iteration)\u001B[39m\n\u001B[32m    171\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m.config.max_retries):\n\u001B[32m    172\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdownload_historic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompanies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_data_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    174\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m LDError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    175\u001B[39m         msg: \u001B[38;5;28mstr\u001B[39m = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError downloading historic data \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, retrying in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdelay\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m seconds\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML/src/data/download.py:192\u001B[39m, in \u001B[36mLSEGDataDownloader.download_historic\u001B[39m\u001B[34m(self, companies, features, raw_data_dir, iteration)\u001B[39m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdownload_historic\u001B[39m(\n\u001B[32m    185\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    186\u001B[39m         companies: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    189\u001B[39m         iteration: \u001B[38;5;28mint\u001B[39m\n\u001B[32m    190\u001B[39m ) -> \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, pd.DataFrame]:\n\u001B[32m    191\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Downloading all fields from a company and join them together\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m     data: DataFrame = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[43mld\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_history\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m            \u001B[49m\u001B[43muniverse\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompanies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m            \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m            \u001B[49m\u001B[43mheader_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mHeaderType\u001B[49m\u001B[43m.\u001B[49m\u001B[43mNAME\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    200\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.standardize_historic_data(data, raw_data_dir, iteration)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/ML/lib/python3.13/site-packages/pandas/core/frame.py:712\u001B[39m, in \u001B[36mDataFrame.__init__\u001B[39m\u001B[34m(self, data, index, columns, dtype, copy)\u001B[39m\n\u001B[32m    708\u001B[39m     allow_mgr = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    709\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m copy:\n\u001B[32m    710\u001B[39m         \u001B[38;5;66;03m# if not copying data, ensure to still return a shallow copy\u001B[39;00m\n\u001B[32m    711\u001B[39m         \u001B[38;5;66;03m# to avoid the result sharing the same Manager\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m712\u001B[39m         data = \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    714\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, (BlockManager, ArrayManager)):\n\u001B[32m    715\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_mgr:\n\u001B[32m    716\u001B[39m         \u001B[38;5;66;03m# GH#52419\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/ML/lib/python3.13/site-packages/pandas/core/internals/managers.py:593\u001B[39m, in \u001B[36mBaseBlockManager.copy\u001B[39m\u001B[34m(self, deep)\u001B[39m\n\u001B[32m    590\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    591\u001B[39m         new_axes = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.axes)\n\u001B[32m--> \u001B[39m\u001B[32m593\u001B[39m res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcopy\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    594\u001B[39m res.axes = new_axes\n\u001B[32m    596\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ndim > \u001B[32m1\u001B[39m:\n\u001B[32m    597\u001B[39m     \u001B[38;5;66;03m# Avoid needing to re-compute these\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/ML/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001B[39m, in \u001B[36mBaseBlockManager.apply\u001B[39m\u001B[34m(self, f, align_keys, **kwargs)\u001B[39m\n\u001B[32m    361\u001B[39m         applied = b.apply(f, **kwargs)\n\u001B[32m    362\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m363\u001B[39m         applied = \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    364\u001B[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001B[32m    366\u001B[39m out = \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).from_blocks(result_blocks, \u001B[38;5;28mself\u001B[39m.axes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/ML/lib/python3.13/site-packages/pandas/core/internals/blocks.py:826\u001B[39m, in \u001B[36mBlock.copy\u001B[39m\u001B[34m(self, deep)\u001B[39m\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    825\u001B[39m     refs = \u001B[38;5;28mself\u001B[39m.refs\n\u001B[32m--> \u001B[39m\u001B[32m826\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplacement\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_mgr_locs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mndim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Downloading static Data",
   "id": "9391c4d8701b8b7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.exceptions import DataValidationError, DataDownloadError\n",
    "\n",
    "with LSEGDataDownloader(config) as downloader:\n",
    "    for i, company_chunk in enumerate(config.companies_static_chunks):\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                print(f\"Downloading static data: for {company_chunk[0]} to {company_chunk[-1]}\")\n",
    "                statdict: dict[str, pd.DataFrame] = downloader.download_static_from(\n",
    "                    companies= company_chunk,\n",
    "                    features= config.static_features,\n",
    "                    raw_data_dir= RAW_DATA_PATH)\n",
    "                for name, frame in statdict.items():\n",
    "                    frame.to_csv(STATIC_DATA_PATH / f\"company-{name}.csv\")\n",
    "                break\n",
    "            except DataValidationError as e:\n",
    "                company_chunk.remove(e.__getcompanies__())\n",
    "                with open(config.removed_companies_file, \"a\") as file:\n",
    "                    file.write(f\"\\n{e.__getcompanies__()}\")\n",
    "                print(f\"Removed {e.__getcompanies__()} Try again.\")\n",
    "                continue\n",
    "        else:\n",
    "            raise DataDownloadError(\"Too many attempts to download static data\", company_chunk)"
   ],
   "id": "d079e5f6a397e8e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Other",
   "id": "d31627d883df066c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "dir1 = config.dataset_dir / \"historic\"\n",
    "dir2 = config.dataset_dir / \"static\"\n",
    "files1: list[str] = [file.name for file in dir1.glob(\"*.csv\")]\n",
    "names1: list[str] = [re.findall(r\"company-+(.*).csv\", file)[0] for file in files1]\n",
    "files2: list[str] = [file.name for file in dir2.glob(\"*.csv\")]\n",
    "names2: list[str] = [re.findall(r\"company-+(.*).csv\", file)[0] for file in files2]\n",
    "not_in1: list[str] = [name for name in names2 if name not in names1]\n",
    "not_in2: list[str] = [name for name in names1 if name not in names2]\n",
    "not_in_both: list[str] = not_in1 + not_in2"
   ],
   "id": "c05e6e49fdb3dfec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_unique(s: pd.DataFrame):\n",
    "    a: np.ndarray = s.to_numpy()\n",
    "    return (a[0] == a).all()\n",
    "without_same_results: pd.DataFrame = all_static_frame[all_static_frame.apply(is_unique, axis=1)]"
   ],
   "id": "4ba2bfd890724108",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
