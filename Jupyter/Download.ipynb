{
 "cells": [
  {
   "metadata": {
    "tags": [
     "Initialization"
    ]
   },
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "Configuration, Logger, Counter and Downloader"
   ],
   "id": "b2f2a0d6e7bd322b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from core.config import Config, split_in_chunks\n",
    "from data.download import LSEGDataDownloader\n",
    "\n",
    "os.environ[\"RD_LIB_CONFIG_PATH\"] = \"/Configuration\"\n",
    "\n",
    "config = Config()\n",
    "logging.basicConfig(\n",
    "        filename=config.log_file,\n",
    "        encoding=\"utf-8\",\n",
    "        level=config.log_level,\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "        datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "logger = logging.getLogger()"
   ],
   "id": "e74ca2369a377986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Downloading time series Data",
   "id": "d595c661a33d1b36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.exceptions import DataValidationError, DataDownloadError\n",
    "\n",
    "removed_companies: list[str] = []\n",
    "with open(config.removed_companies_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        removed_companies.append(line.strip())\n",
    "\n",
    "companies: list[str] = [company for company in config.companies if removed_companies not in config.companies]\n",
    "companies_chunks: list[list[str]] = split_in_chunks(\n",
    "    companies,\n",
    "    chunk_size=config.companies_chunk_size_historic,\n",
    "    chunk_limit=config.chunk_limit\n",
    ")\n",
    "\n",
    "with (LSEGDataDownloader(config) as downloader):\n",
    "    for i, company_chunk in enumerate(companies_chunks[11:]):\n",
    "        for attempt in range(100):\n",
    "            try:\n",
    "                print(f\"Downloading historic data: for {company_chunk[0]} to {company_chunk[-1]}\")\n",
    "                data: pd.DataFrame = downloader.download_historic_from(company_chunk, config.historic_features)\n",
    "                standardized_histordict: dict[str, pd.DataFrame] = downloader.standardize_historic_data(data, i)\n",
    "                del data\n",
    "                for key, new_df in standardized_histordict.items():\n",
    "                    new_df.to_csv(config.filtered_historic_dir / f\"company-{key}.csv\")\n",
    "                break\n",
    "            except DataValidationError as e:\n",
    "                company_chunk.remove(e.__getcompanies__())\n",
    "                with open(config.removed_companies_file, \"a\") as file:\n",
    "                    file.write(f\"\\n{e.__getcompanies__()}\")\n",
    "                print(f\"Removed {e.__getcompanies__()} Try again.\")\n",
    "                continue\n",
    "        else:\n",
    "            raise DataDownloadError(\"Too many attempts to download historic data\", company_chunk)"
   ],
   "id": "8ea3a8e6266ce6f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Downloading static Data",
   "id": "9391c4d8701b8b7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.exceptions import DataValidationError, DataDownloadError\n",
    "\n",
    "removed_companies: list[str] = []\n",
    "with open(config.removed_companies_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        removed_companies.append(line.strip())\n",
    "\n",
    "companies: list[str] = [company for company in config.companies if removed_companies not in config.companies]\n",
    "companies_chunks: list[list[str]] = split_in_chunks(\n",
    "    companies,\n",
    "    chunk_size=config.companies_chunk_size_static,\n",
    "    chunk_limit=config.chunk_limit\n",
    ")\n",
    "\n",
    "with LSEGDataDownloader(config) as downloader:\n",
    "    for i, company_chunk in enumerate(companies_chunks):\n",
    "        for attempt in range(100):\n",
    "            try:\n",
    "                print(f\"Downloading static data: for {company_chunk[0]} to {company_chunk[-1]}\")\n",
    "                statdict: dict[str, pd.DataFrame] = downloader.download_static_from(company_chunk, config.static_features)\n",
    "                for name, frame in statdict.items():\n",
    "                    frame.to_csv(config.filtered_static_dir / f\"company-{name}.csv\")\n",
    "                break\n",
    "            except DataValidationError as e:\n",
    "                company_chunk.remove(e.__getcompanies__())\n",
    "                with open(config.removed_companies_file, \"a\") as file:\n",
    "                    file.write(f\"\\n{e.__getcompanies__()}\")\n",
    "                print(f\"Removed {e.__getcompanies__()} Try again.\")\n",
    "                continue\n",
    "        else:\n",
    "            raise DataDownloadError(\"Too many attempts to download static data\", company_chunk)"
   ],
   "id": "d079e5f6a397e8e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Other",
   "id": "d31627d883df066c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "dir1 = config.dataset_dir / \"historic\"\n",
    "dir2 = config.dataset_dir / \"static\"\n",
    "files1: list[str] = [file.name for file in dir1.glob(\"*.csv\")]\n",
    "names1: list[str] = [re.findall(r\"company-+(.*).csv\", file)[0] for file in files1]\n",
    "files2: list[str] = [file.name for file in dir2.glob(\"*.csv\")]\n",
    "names2: list[str] = [re.findall(r\"company-+(.*).csv\", file)[0] for file in files2]\n",
    "not_in1: list[str] = [name for name in names2 if name not in names1]\n",
    "not_in2: list[str] = [name for name in names1 if name not in names2]\n",
    "not_in_both: list[str] = not_in1 + not_in2"
   ],
   "id": "c05e6e49fdb3dfec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_unique(s: pd.DataFrame):\n",
    "    a: np.ndarray = s.to_numpy()\n",
    "    return (a[0] == a).all()\n",
    "without_same_results: pd.DataFrame = all_static_frame[all_static_frame.apply(is_unique, axis=1)]"
   ],
   "id": "4ba2bfd890724108",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
